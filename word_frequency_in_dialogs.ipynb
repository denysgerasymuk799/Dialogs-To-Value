{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_transformation import prepare_dialogs, if_in_date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas~=0.25.3 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 1)) (0.25.3)\n",
      "Requirement already satisfied: numpy~=1.17.0 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 2)) (1.17.5)\n",
      "Requirement already satisfied: nltk~=3.4 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 3)) (3.5)\n",
      "Requirement already satisfied: word2number~=1.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 4)) (1.1)\n",
      "Requirement already satisfied: num2words~=0.5.10 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 5)) (0.5.10)\n",
      "Requirement already satisfied: pymorphy2~=0.8 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 6)) (0.8)\n",
      "Requirement already satisfied: pip~=20.2.2 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 7)) (20.2.2)\n",
      "Requirement already satisfied: tokenize_uk~=0.2.0 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: matplotlib~=3.3.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 9)) (3.3.1)\n",
      "Requirement already satisfied: scikit-learn~=0.23.2 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 10)) (0.23.2)\n",
      "Requirement already satisfied: nlpcube~=0.1.0.8 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 11)) (0.1.0.8)\n",
      "Requirement already satisfied: stop-words~=2018.7.23 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from -r requirements.txt (line 12)) (2018.7.23)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from pandas~=0.25.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from pandas~=0.25.3->-r requirements.txt (line 1)) (2020.1)\n",
      "Requirement already satisfied: click in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nltk~=3.4->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: joblib in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nltk~=3.4->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: regex in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nltk~=3.4->-r requirements.txt (line 3)) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nltk~=3.4->-r requirements.txt (line 3)) (4.48.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from num2words~=0.5.10->-r requirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from pymorphy2~=0.8->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from pymorphy2~=0.8->-r requirements.txt (line 6)) (2.4.393442.3710985)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from tokenize_uk~=0.2.0->-r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from matplotlib~=3.3.1->-r requirements.txt (line 9)) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from matplotlib~=3.3.1->-r requirements.txt (line 9)) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from matplotlib~=3.3.1->-r requirements.txt (line 9)) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from matplotlib~=3.3.1->-r requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from matplotlib~=3.3.1->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from scikit-learn~=0.23.2->-r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from scikit-learn~=0.23.2->-r requirements.txt (line 10)) (1.5.2)\n",
      "Requirement already satisfied: dyNET in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (2.1)\n",
      "Requirement already satisfied: xmltodict==0.11.0 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: nose2==0.7.3 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (0.7.3)\n",
      "Requirement already satisfied: flask in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (4.9.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (2.24.0)\n",
      "Requirement already satisfied: future>=0.16.0 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (0.18.2)\n",
      "Requirement already satisfied: Cython>=0.28.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (0.29.14)\n",
      "Requirement already satisfied: coverage>=4.4.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from nose2==0.7.3->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (5.2.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from flask->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from flask->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (2.11.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from flask->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (1.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from beautifulsoup4->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from requests>=2.18.4->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from requests>=2.18.4->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from requests>=2.18.4->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages (from Jinja2>=2.10.1->flask->nlpcube~=0.1.0.8->-r requirements.txt (line 11)) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: add this packages to the requirements.txt file\n",
    "\n",
    "# !pip install -U nlpcube\n",
    "# !pip install stop-words\n",
    "# !pip install git+https://github.com/Desklop/Uk_Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: rename this method, calculate is to generic word, name should give you information about method\n",
    "def get_sorted_word_frequency(dialog_data, start_date, end_date, user_id_get_msg, dialog_id, path_to_processed_files):\n",
    "    \"\"\"\n",
    "    :param dialog_data: a pandas dataframe of dialogs\n",
    "    :param start_date: datetime type, from what time start to analyse msgs\n",
    "    :param end_date: datetime type, to what time to analyse msgs\n",
    "    :param user_id_get_msg: a user chat id, who msgs to analyse or \"all\" - for all participants of the dialog\n",
    "    :param dialog_id: str\n",
    "    :param path_to_processed_files: path to processed_dialog_files dir\n",
    "    :return: a sorted df - sorted dict of user_id_get_msg most used words\n",
    "    \"\"\"\n",
    "    DF = {}\n",
    "    file_text = \"\"\n",
    "\n",
    "    for row in dialog_data.index:\n",
    "        if user_id_get_msg != \"all\":\n",
    "            if dialog_data[\"from_id\"][row] != user_id_get_msg:\n",
    "                continue\n",
    "\n",
    "        if if_in_date_range(dialog_data[\"date\"][row][:-6], start_date, end_date) == 'Dialog after end_date':\n",
    "            continue\n",
    "\n",
    "        elif if_in_date_range(dialog_data[\"date\"][row][:-6], start_date, end_date):\n",
    "            if not pd.isnull(dialog_data[\"message\"][row]):\n",
    "                file_text += dialog_data[\"message\"][row] + \" \"\n",
    "                for w in dialog_data[\"message\"][row].split():\n",
    "                    try:\n",
    "                        DF[w] += 1\n",
    "                    except:\n",
    "                        DF[w] = 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    with open(path_to_processed_files + \"{}/1_{}.txt\".format(dialog_id, dialog_id), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_text)\n",
    "\n",
    "    DF_sorted = {k: v for k, v in sorted(DF.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    return DF_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) change a dialog_id, which you want to investigate\n",
    "dialog_id = \"138918380\"\n",
    "\n",
    "# 2) change user_id_get_msg\n",
    "# int type - user chat id, who massages you want to analyse,\n",
    "# you can find it in <dialog_id>.csv\n",
    "# str type \"all\" if you want to analyse msgs of all users of the dialog\n",
    "user_id_get_msg = 511986933\n",
    "dialog_path = \"data/dialogs/\"\n",
    "prep_path = 'data/prepared_dialogs/'\n",
    "dialog_lang = \"ua\"\n",
    "path_to_logs = os.path.join(\"logs\", \"project_logs.log\")\n",
    "\n",
    "# TODO: path_to_static, static what? you should rename this folder with respect to its purpose!\n",
    "path_to_processed_files = \"data/processed_dialog_files/\"\n",
    "\n",
    "# TODO: rename start_date and end_date -> start_date and end_date\n",
    "# 3) change to date range in what you want to analyse messages of user_id_get_msg - from start_date to end_date;\n",
    "# format \"%Y-%m-%d %H:%M:%S\"\n",
    "start_date = datetime.datetime(2017, 7, 9, 0, 0, 0)\n",
    "end_date = datetime.datetime(2020, 8, 10, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# TODO: add log file path to global var\n",
    "logging.basicConfig(filename=path_to_logs, level=0)\n",
    "logging.info(\"starting logs for tf_idf_dialogs\")\n",
    "\n",
    "# TODO: move instructions in the README file\n",
    "\n",
    "# 0) in console input: import nltk; nltk.download()\n",
    "#\n",
    "# Install (or update) NLP-Cube with:\n",
    "# pip3 install -U nlpcube\n",
    "#\n",
    "# use telegram-data-collection/0_download_dialogs_list.py and 1_download_dialogs_data.py\n",
    "# to get some files to analyse\n",
    "# files SHOULD be in data/dialogs and data/dialogs_meta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_to_processed_files):\n",
    "    os.mkdir(path_to_processed_files)\n",
    "    \n",
    "if not os.path.exists(path_to_processed_files + dialog_id):\n",
    "    os.mkdir(path_to_processed_files + dialog_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a method to detect dialog language\n",
    "\n",
    "# 4) change \"ua\" to a language of your dialog (\"ua\", \"ru\" or \"en\")\n",
    "# if you write \"ua\" or \"ru\" - dialog will be cleaned from stop_words_ua + stop_words_ru + stop_words_en\n",
    "# so do not bother about these languages if you can not write an exactly language of the dialog\n",
    "\n",
    "# TODO: move lang to the variable\n",
    "\n",
    "prepare_dialogs(dialog_id, dialog_path, prep_path, start_date,\n",
    "                end_date, dialog_lang, \"words_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialog_data = pd.read_csv(prep_path + dialog_id + \".csv\")\n",
    "dialog_data = pd.read_csv(\"data/489299513.csv\")\n",
    "\n",
    "DF_sorted = get_sorted_word_frequency(dialog_data, start_date, end_date, user_id_get_msg, dialog_id, path_to_processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'і': 3, 'не': 3, 'тут': 2, 'привіт': 2, 'ніч': 2, 'Привіт': 2, 'якщо': 2, 'мікрофон': 2, 'на': 2, 'відео': 2, 'велкам': 1, 'дейлік': 1, 'https://meet.google.com/zqi-ohib-ddv': 1, 'нічо': 1, 'цікавого': 1, 'перший': 1, 'можеш': 1, 'скіпати': 1, 'фаєрфлай': 1, 'чи': 1, 'дейлік?': 1, 'Дякую': 1, 'Оце': 1, 'тімворк': 1, 'Супер': 1, 'Я': 1, 'їхав': 1, 'у': 1, 'львів': 1, 'всю': 1, 'Проспав': 1, 'будильник': 1, 'Та': 1, 'була': 1, 'важка': 1, 'будеш': 1, 'відеомесаги': 1, 'а': 1, 'натиснеш': 1, 'раз': 1, 'то': 1, 'переключиться': 1, 'запишеш': 1, 'голосовуху': 1, 'є': 1, 'натисни': 1, 'тримай': 1, 'та': 1, 'де': 1, 'кнопка': 1, 'відправити': 1, 'ти': 1, 'нічого': 1, 'друкуєш': 1, 'пишеш': 1, 'аа': 1, 'сорі,': 1, 'яке': 1, 'в': 1, 'кругому': 1, 'віконці?': 1, 'я': 1, 'поки': 1, 'вшарив': 1, 'дійсно': 1, 'трохи': 1, 'заліп': 1, 'попередньому': 1, '=)': 1, 'дякую': 1}\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_processed_files + \"{}/words_frequency1_{}.json\".format(dialog_id, dialog_id), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(DF_sorted, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# 5) after work of this module look at results in static/<dialog_id> dir,\n",
    "# use command under to make a wordcloud, to understand its parameters use - wordcloud_cli --help\n",
    "# Example: wordcloud_cli --text static/<dialog_id>/1_<dialog_id>.txt --mask static/logo_telegram.jpg --imagefile static/my_words_nazar2.png\n",
    "print(DF_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}