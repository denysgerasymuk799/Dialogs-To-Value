{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cube.api import Cube\n",
    "from utils.text_data_transformation import transform_raw_data\n",
    "from utils.dialog_manipulation import prepare_dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      dialog_id                                               text\n0             1                                                   \n1             2   —Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...\n2             3                                                 üòÇüòÇ\n3             4                                                   \n4             5           –ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....\n...         ...                                                ...\n4202       4203                                     –¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏\n4203       4204                                              —Ç—Ä–µ–±–∞\n4204       4205    –¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...\n4205       4206    –Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...\n4206       4207                                       —è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ\n\n[4207 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialog_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>—Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>üòÇüòÇ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>–ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4202</th>\n      <td>4203</td>\n      <td>–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏</td>\n    </tr>\n    <tr>\n      <th>4203</th>\n      <td>4204</td>\n      <td>—Ç—Ä–µ–±–∞</td>\n    </tr>\n    <tr>\n      <th>4204</th>\n      <td>4205</td>\n      <td>–¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...</td>\n    </tr>\n    <tr>\n      <th>4205</th>\n      <td>4206</td>\n      <td>–Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...</td>\n    </tr>\n    <tr>\n      <th>4206</th>\n      <td>4207</td>\n      <td>—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ</td>\n    </tr>\n  </tbody>\n</table>\n<p>4207 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0) Firstly, use data_transformation.py to get \"subdialogs_{id}.csv\" files\n",
    "\n",
    "\n",
    "# or DIALOG_IDS = [-1] for all in dir\n",
    "DIALOG_IDS = [-1]\n",
    "# DIALOG_IDS = [\"138918380\", \"470409323\", \"347963763\", \"850978724\", \"511986933\"]\n",
    "path_to_dialogs_frame = os.path.join(\"data\", \"new_type_dialogs_prepared2\", \"general_df.csv\")\n",
    "path_to_save_result = os.path.join(\"data\", \"processed_dialog_files\", \"subdialog_keywords.csv\")\n",
    "\n",
    "# flag_get_all = 0\n",
    "# if DIALOG_IDS[0] == -1:\n",
    "#     DIALOG_IDS = os.listdir(path_to_prepared_dialogs)\n",
    "#     flag_get_all = 1\n",
    "\n",
    "frames = []\n",
    "df_idf = pd.DataFrame()\n",
    "general_n_subdialogs = 0\n",
    "\n",
    "\n",
    "data_for_df = []\n",
    "data = pd.read_csv(path_to_dialogs_frame)\n",
    "\n",
    "n_subdialog = 0\n",
    "dialog_dict, dialog_text = {}, ''\n",
    "for index, row in data.iterrows():\n",
    "    if n_subdialog != row.subdialog_id:\n",
    "        n_subdialog += 1\n",
    "        general_n_subdialogs += 1\n",
    "        if n_subdialog != 1:\n",
    "            dialog_dict[\"text\"] = dialog_text\n",
    "            data_for_df.append(dialog_dict)\n",
    "\n",
    "        dialog_dict = {}\n",
    "        dialog_text = ''\n",
    "        dialog_dict[\"dialog_id\"] = general_n_subdialogs\n",
    "\n",
    "    if not pd.isnull(row.preprocessed_message):\n",
    "        dialog_text += \" \" + row.preprocessed_message\n",
    "\n",
    "df_idf = pd.DataFrame(data_for_df)\n",
    "df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1        —Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...\n2                                                      üòÇüòÇ\n4                –ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....\n5        —Ç–∏ –∑–∞–∫–∏–Ω—É–≤ –∑–∞—Ä–∞–∑, —â–æ–± –≤—ñ–Ω —Ä–æ–∑—ñ–≥—Ä—ñ–≤—Å—è —Ç—Ä–æ—Ö–∏?) ...\n6        –Ω–µ –≤—ñ–¥–ø–∏—Å—É—î –º–µ–Ω—ñ(( –ë–æ–π–∫–æ —Å—é–¥—è—á–∏ –∑ –Ü–Ω—Å—Ç–∞–≥—Ä–∞–º—É ...\n                              ...                        \n4202                                       –¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏\n4203                                                —Ç—Ä–µ–±–∞\n4204      –¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...\n4205      –Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...\n4206                                         —è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ\nName: text, Length: 3337, dtype: object"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'].replace('', np.nan, inplace=True)\n",
    "df_idf.dropna(subset = [\"text\"], inplace=True)\n",
    "\n",
    "# if you have untransformed_data so uncomment it !!!!!!!!!!!!!\n",
    "# if lang == \"ua\":\n",
    "#     cube = Cube(verbose=True)\n",
    "#     cube.load(\"uk\")\n",
    "# df_idf['text'] = df_idf['text'].apply(lambda x: transform_raw_data(x, lang, \"words_frequency\", cube))\n",
    "\n",
    "#show the first 'text'\n",
    "df_idf['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['_io', 'analysis', 'dialogs', 'dicts', 'encoding', 'mode', 'my_work', 'name', 'programming', 'telegram', 'textiowrapper', 'txt', 'ukrainian_stopwords', 'utf'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"load stop words \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(os.getcwd(), \"..\", \"dicts\", \"ukrainian_stopwords.txt\"), \"r\", encoding=\"utf-8\") as file:\n",
    "            stop_words = str(file).strip().split()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        with open(os.path.join(os.getcwd(), \"dicts\", \"ukrainian_stopwords.txt\"), \"r\",\n",
    "                  encoding=\"utf-8\") as file:\n",
    "            stop_words = str(file).strip().split()\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words()\n",
    "\n",
    "#get the text column\n",
    "docs=df_idf['text'].tolist()\n",
    "\n",
    "#create a vocabulary of words,\n",
    "#ignore words that appear in 85% of documents,\n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(3337, 10000)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000, ngram_range=(2, 2))\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# get text docs into a list\n",
    "docs_text=df_idf['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Text=====\n",
      " üòÇüòÇ\n",
      "\n",
      "===Keywords===\n"
     ]
    }
   ],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "\n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "\n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "    return results\n",
    "\n",
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_text[1]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Text=====\")\n",
    "print(docs_text[1])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# put the common code into several methods\n",
    "def get_keywords(idx):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_text[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Text=====\")\n",
    "    print(docs_text[idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result df\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                    doc  \\\n0      —Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...   \n1                                                    üòÇüòÇ   \n2              –ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....   \n3      —Ç–∏ –∑–∞–∫–∏–Ω—É–≤ –∑–∞—Ä–∞–∑, —â–æ–± –≤—ñ–Ω —Ä–æ–∑—ñ–≥—Ä—ñ–≤—Å—è —Ç—Ä–æ—Ö–∏?) ...   \n4      –Ω–µ –≤—ñ–¥–ø–∏—Å—É—î –º–µ–Ω—ñ(( –ë–æ–π–∫–æ —Å—é–¥—è—á–∏ –∑ –Ü–Ω—Å—Ç–∞–≥—Ä–∞–º—É ...   \n...                                                 ...   \n3332                                     –¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏   \n3333                                              —Ç—Ä–µ–±–∞   \n3334    –¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...   \n3335    –Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...   \n3336                                       —è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ   \n\n                                               keywords  \n0     {'—è–∫–∏–π —Ç–∏': 0.302, '—Ç—É—Ç –º–∞–≤': 0.302, '—Ç–∏ –≥–∞—Ä—è—á...  \n1                                                    {}  \n2     {'—à–∏–±–∫–æ –≥–∞—Ä—è—á–µ': 0.408, '—Ç–æ–±—ñ —Å–∫–∞–∂—É': 0.408, '...  \n3     {'—â–æ–± –≤—ñ–Ω': 0.176, '—Ü–µ 15': 0.176, '—Ö—Ç–æ—Å—å –∑–∞—Ö–æ...  \n4     {'—ñ–Ω—Å—Ç–∞–≥—Ä–∞–º—É –ø—Ä–∏–≤—ñ—Ç': 0.237, '—Ö—Ç–æ—Å—å –º–∞—î': 0.23...  \n...                                                 ...  \n3332                            {'–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏': 1.0}  \n3333                                                 {}  \n3334  {'—Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω': 0.51, '—Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π': 0.5...  \n3335  {'—è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è': 0.456, '—Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ'...  \n3336                              {'—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ': 1.0}  \n\n[3337 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>—Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...</td>\n      <td>{'—è–∫–∏–π —Ç–∏': 0.302, '—Ç—É—Ç –º–∞–≤': 0.302, '—Ç–∏ –≥–∞—Ä—è—á...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>üòÇüòÇ</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>–ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....</td>\n      <td>{'—à–∏–±–∫–æ –≥–∞—Ä—è—á–µ': 0.408, '—Ç–æ–±—ñ —Å–∫–∞–∂—É': 0.408, '...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>—Ç–∏ –∑–∞–∫–∏–Ω—É–≤ –∑–∞—Ä–∞–∑, —â–æ–± –≤—ñ–Ω —Ä–æ–∑—ñ–≥—Ä—ñ–≤—Å—è —Ç—Ä–æ—Ö–∏?) ...</td>\n      <td>{'—â–æ–± –≤—ñ–Ω': 0.176, '—Ü–µ 15': 0.176, '—Ö—Ç–æ—Å—å –∑–∞—Ö–æ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>–Ω–µ –≤—ñ–¥–ø–∏—Å—É—î –º–µ–Ω—ñ(( –ë–æ–π–∫–æ —Å—é–¥—è—á–∏ –∑ –Ü–Ω—Å—Ç–∞–≥—Ä–∞–º—É ...</td>\n      <td>{'—ñ–Ω—Å—Ç–∞–≥—Ä–∞–º—É –ø—Ä–∏–≤—ñ—Ç': 0.237, '—Ö—Ç–æ—Å—å –º–∞—î': 0.23...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3332</th>\n      <td>–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏</td>\n      <td>{'–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏': 1.0}</td>\n    </tr>\n    <tr>\n      <th>3333</th>\n      <td>—Ç—Ä–µ–±–∞</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>3334</th>\n      <td>–¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...</td>\n      <td>{'—Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω': 0.51, '—Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π': 0.5...</td>\n    </tr>\n    <tr>\n      <th>3335</th>\n      <td>–Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...</td>\n      <td>{'—è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è': 0.456, '—Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ'...</td>\n    </tr>\n    <tr>\n      <th>3336</th>\n      <td>—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ</td>\n      <td>{'—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ': 1.0}</td>\n    </tr>\n  </tbody>\n</table>\n<p>3337 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate tf-idf for all documents in your list. docs_text has 500 documents\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform(docs_text))\n",
    "\n",
    "results=[]\n",
    "for i in range(tf_idf_vector.shape[0]):\n",
    "\n",
    "    # get vector for a single document\n",
    "    curr_vector=tf_idf_vector[i]\n",
    "\n",
    "    #sort the tf-idf vector by descending order of scores\n",
    "    sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "    results.append(keywords)\n",
    "\n",
    "df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
    "for result in results:\n",
    "    result2 = {k: v for k, v in sorted(result.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "df.to_csv(path_to_save_result, index=False)\n",
    "print(\"result df\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}