{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from cube.api import Cube\n",
    "from utils.text_data_transformation import transform_raw_data\n",
    "from utils.dialog_manipulation import prepare_dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      dialog_id title                                               body\n0             1                                                         \n1             2         —Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...\n2             3                                                       üòÇüòÇ\n3             4                                                         \n4             5                 –ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....\n...         ...   ...                                                ...\n4202       4203                                           –¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏\n4203       4204                                                    —Ç—Ä–µ–±–∞\n4204       4205          –¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...\n4205       4206          –Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...\n4206       4207                                             —è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ\n\n[4207 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dialog_id</th>\n      <th>title</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td></td>\n      <td>—Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td></td>\n      <td>üòÇüòÇ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td></td>\n      <td>–ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4202</th>\n      <td>4203</td>\n      <td></td>\n      <td>–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏</td>\n    </tr>\n    <tr>\n      <th>4203</th>\n      <td>4204</td>\n      <td></td>\n      <td>—Ç—Ä–µ–±–∞</td>\n    </tr>\n    <tr>\n      <th>4204</th>\n      <td>4205</td>\n      <td></td>\n      <td>–¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...</td>\n    </tr>\n    <tr>\n      <th>4205</th>\n      <td>4206</td>\n      <td></td>\n      <td>–Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...</td>\n    </tr>\n    <tr>\n      <th>4206</th>\n      <td>4207</td>\n      <td></td>\n      <td>—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ</td>\n    </tr>\n  </tbody>\n</table>\n<p>4207 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0) Firstly, use data_transformation.py to get \"subdialogs_{id}.csv\" files\n",
    "\n",
    "\n",
    "# or DIALOG_IDS = [-1] for all in dir\n",
    "DIALOG_IDS = [-1]\n",
    "# DIALOG_IDS = [\"138918380\", \"470409323\", \"347963763\", \"850978724\", \"511986933\"]\n",
    "path_to_dialogs_frame = os.path.join(\"data\", \"new_type_dialogs_prepared2\", \"general_df.csv\")\n",
    "path_to_save_result = os.path.join(\"data\", \"processed_dialog_files\", \"subdialog_keywords.csv\")\n",
    "\n",
    "# flag_get_all = 0\n",
    "# if DIALOG_IDS[0] == -1:\n",
    "#     DIALOG_IDS = os.listdir(path_to_prepared_dialogs)\n",
    "#     flag_get_all = 1\n",
    "\n",
    "frames = []\n",
    "df_idf = pd.DataFrame()\n",
    "general_n_subdialogs = 0\n",
    "\n",
    "\n",
    "data_for_df = []\n",
    "data = pd.read_csv(path_to_dialogs_frame)\n",
    "\n",
    "n_subdialog = 0\n",
    "dialog_dict, dialog_text = {}, ''\n",
    "for index, row in data.iterrows():\n",
    "    if n_subdialog != row.subdialog_id:\n",
    "        n_subdialog += 1\n",
    "        general_n_subdialogs += 1\n",
    "        if n_subdialog != 1:\n",
    "            dialog_dict[\"body\"] = dialog_text\n",
    "            data_for_df.append(dialog_dict)\n",
    "\n",
    "        dialog_dict = {}\n",
    "        dialog_text = ''\n",
    "        # TODO: \"body\" delete from tf_idf\n",
    "        # TODO: \"title\" delete from tf_idf\n",
    "        dialog_dict[\"dialog_id\"] = general_n_subdialogs\n",
    "        dialog_dict[\"title\"] = \"\"\n",
    "\n",
    "    if not pd.isnull(row.preprocessed_message):\n",
    "        dialog_text += \" \" + row.preprocessed_message\n",
    "\n",
    "df_idf = pd.DataFrame(data_for_df)\n",
    "df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 1) write your number of rows to analyse. max == 695,\n",
    "# # but to clean and analyse 30 rows is spending 6 minutes\n",
    "# path_to_articles = os.path.join('data', 'processed_dialog_files', 'ukr_articles')\n",
    "# data = pd.read_csv(path_to_articles, sep = ';', header = None)\n",
    "# df_idf = pd.DataFrame(data[:20])\n",
    "#\n",
    "# df_idf.columns = [\"id\", \"title\", \"en_title\", \"body\", \"date\", \"source_url\",\n",
    "#                   \"article_url\"]\n",
    "#\n",
    "# print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "# print(\"Number of questions,columns=\",df_idf.shape)\n",
    "# df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' üòÇüòÇ'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_idf.dropna(subset = [\"title\"], inplace=True)\n",
    "df_idf.dropna(subset = [\"body\"], inplace=True)\n",
    "\n",
    "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
    "\n",
    "# if you have untransformed_data so uncomment it !!!!!!!!!!!!!\n",
    "# if lang == \"ua\":\n",
    "#     cube = Cube(verbose=True)\n",
    "#     cube.load(\"uk\")\n",
    "# df_idf['text'] = df_idf['text'].apply(lambda x: transform_raw_data(x, lang, \"words_frequency\", cube))\n",
    "\n",
    "#show the first 'text'\n",
    "df_idf['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\envs\\venv_telegram_analysis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['_io', 'analysis', 'dialogs', 'dicts', 'encoding', 'mode', 'my_work', 'name', 'programming', 'telegram', 'textiowrapper', 'txt', 'ukrainian_stopwords', 'utf'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"load stop words \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(os.getcwd(), \"..\", \"dicts\", \"ukrainian_stopwords.txt\"), \"r\", encoding=\"utf-8\") as file:\n",
    "            stop_words = str(file).strip().split()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        with open(os.path.join(os.getcwd(), \"dicts\", \"ukrainian_stopwords.txt\"), \"r\",\n",
    "                  encoding=\"utf-8\") as file:\n",
    "            stop_words = str(file).strip().split()\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words()\n",
    "\n",
    "#get the text column\n",
    "docs=df_idf['text'].tolist()\n",
    "\n",
    "#create a vocabulary of words,\n",
    "#ignore words that appear in 85% of documents,\n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(4207, 10000)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000, ngram_range=(2, 2))\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# get test docs into a list\n",
    "docs_test=df_idf['text'].tolist()\n",
    "docs_title=df_idf['title'].tolist()\n",
    "docs_body=df_idf['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "\n",
      "\n",
      "=====Body=====\n",
      "\n",
      "\n",
      "===Keywords===\n"
     ]
    }
   ],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "\n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "\n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "    return results\n",
    "\n",
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_test[0]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(docs_title[0])\n",
    "print(\"\\n=====Body=====\")\n",
    "print(docs_body[0])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# put the common code into several methods\n",
    "def get_keywords(idx):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(docs_title[idx])\n",
    "    print(\"\\n=====Body=====\")\n",
    "    print(docs_body[idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result df\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                    doc  \\\n0                                                         \n1      —Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...   \n2                                                    üòÇüòÇ   \n3                                                         \n4              –ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....   \n...                                                 ...   \n4202                                     –¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏   \n4203                                              —Ç—Ä–µ–±–∞   \n4204    –¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...   \n4205    –Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...   \n4206                                       —è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ   \n\n                                               keywords  \n0                                                    {}  \n1     {'—è–∫–∏–π —Ç–∏': 0.302, '—Ç—É—Ç –º–∞–≤': 0.302, '—Ç–∏ –≥–∞—Ä—è—á...  \n2                                                    {}  \n3                                                    {}  \n4     {'—à–∏–±–∫–æ –≥–∞—Ä—è—á–µ': 0.408, '—Ç–æ–±—ñ —Å–∫–∞–∂—É': 0.408, '...  \n...                                                 ...  \n4202                            {'–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏': 1.0}  \n4203                                                 {}  \n4204  {'—Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω': 0.51, '—Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π': 0.5...  \n4205  {'—è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è': 0.456, '—Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ'...  \n4206                              {'—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ': 1.0}  \n\n[4207 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>—Ç—É—Ç –º–∞–≤ –±–∏ –±—É—Ç–∏ –∂–∞—Ä—Ç –ø—Ä–æ —Ç–µ —è–∫–∏–π —Ç–∏ –≥–∞—Ä—è—á–∏–π, ...</td>\n      <td>{'—è–∫–∏–π —Ç–∏': 0.302, '—Ç—É—Ç –º–∞–≤': 0.302, '—Ç–∏ –≥–∞—Ä—è—á...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>üòÇüòÇ</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>–ù—É —Ç–∞–º –Ω–µ —à–∏–±–∫–æ –≥–∞—Ä—è—á–µ, —è —Ç–æ–±—ñ —Å–∫–∞–∂—É....</td>\n      <td>{'—à–∏–±–∫–æ –≥–∞—Ä—è—á–µ': 0.408, '—Ç–æ–±—ñ —Å–∫–∞–∂—É': 0.408, '...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4202</th>\n      <td>–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏</td>\n      <td>{'–¥–æ–±—Ä–µ –¥—è–∫—É–≤–∞—Ç–∏': 1.0}</td>\n    </tr>\n    <tr>\n      <th>4203</th>\n      <td>—Ç—Ä–µ–±–∞</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>4204</th>\n      <td>–¥–æ–±—Ä–∏–π —Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω –∑–∞–∑–¥–∞–ª–µ...</td>\n      <td>{'—Ç–µ—Å—Ç –º—ñ–∫—Ä–æ—Ñ–æ–Ω': 0.51, '—Ä–∞–Ω–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π': 0.5...</td>\n    </tr>\n    <tr>\n      <th>4205</th>\n      <td>–Ω–µ–≤—ñ–¥–æ–º–æ —è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞—Ä–∞—Ö—É–≤–∞—Ç–∏—Å—è...</td>\n      <td>{'—è–∫–∏–π—Å—å —Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è': 0.456, '—Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –º–æ–∂–µ'...</td>\n    </tr>\n    <tr>\n      <th>4206</th>\n      <td>—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ</td>\n      <td>{'—è–∫—â–æ —Å–∫–ª–∞–¥–Ω–æ': 1.0}</td>\n    </tr>\n  </tbody>\n</table>\n<p>4207 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate tf-idf for all documents in your list. docs_test has 500 documents\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform(docs_test))\n",
    "\n",
    "results=[]\n",
    "for i in range(tf_idf_vector.shape[0]):\n",
    "\n",
    "    # get vector for a single document\n",
    "    curr_vector=tf_idf_vector[i]\n",
    "\n",
    "    #sort the tf-idf vector by descending order of scores\n",
    "    sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "\n",
    "    results.append(keywords)\n",
    "\n",
    "df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
    "for result in results:\n",
    "    result2 = {k: v for k, v in sorted(result.items(), key=lambda item: item[1], reverse=True)}\n",
    "    # print(result2)\n",
    "\n",
    "df.to_csv(path_to_save_result, index=False)\n",
    "print(\"result df\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}