{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from textblob import TextBlob\n",
    "\n",
    "from config import PATH_TO_SAVE_GENERAL_DF\n",
    "from utils.dict_manipulation import get_ua_tonality_dict_combined, get_ru_tonality_dict\n",
    "from utils.date import get_day_and_hour, get_week_day_from_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_msg_sentiment(msg, lang):\n",
    "    \"\"\"\n",
    "    Calculate dictionary-base sentiment of a particular message.\n",
    "    Return average sentiment of a message.\n",
    "\n",
    "    Works for en, ua, ru\n",
    "    \"\"\"\n",
    "    if not isinstance(msg, str):\n",
    "        return 0\n",
    "\n",
    "    if lang == \"en\":\n",
    "        return round(TextBlob(msg).sentiment.polarity, 4)\n",
    "    elif lang == \"ua\":\n",
    "        tonality_dict = tonality_dict_ua\n",
    "    elif lang == \"ru\":\n",
    "        tonality_dict = tonality_dict_ru\n",
    "\n",
    "    tokenized_msg = msg.split()\n",
    "    overall_sentiment = 0\n",
    "    words_num = 0\n",
    "\n",
    "    for token in tokenized_msg:\n",
    "        token_sentiment = tonality_dict.get(token.lower(), 0)\n",
    "        if token_sentiment:\n",
    "            overall_sentiment += token_sentiment\n",
    "            words_num += 1\n",
    "\n",
    "    if not words_num:\n",
    "        return 0\n",
    "\n",
    "    avg_sentiment = overall_sentiment / words_num\n",
    "\n",
    "    return avg_sentiment\n",
    "\n",
    "\n",
    "def add_dialog_sentiment(data, save_to_file=False,\n",
    "                         save_path=\"data/processed_dialog_files/general_dialogs_sentiment.csv\"):\n",
    "    \"\"\"\n",
    "    Add sentiment for each message in a dialog,\n",
    "    and add it in a new column for a particular message,\n",
    "    return new dataframe\n",
    "\n",
    "    Supported languages: eng, ru, ua\n",
    "    \"\"\"\n",
    "    data['msg_sentiment'] = data.apply(lambda x: calculate_msg_sentiment(x['preprocessed_message'],\n",
    "                                                                         x['dialog_language']),\n",
    "                                       axis=1)\n",
    "\n",
    "    if save_to_file:\n",
    "        data.to_csv(save_path, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_avg_subdialog_sentiment(data):\n",
    "    \"\"\"\n",
    "    Calculate average sentiment for each subdialog in a dataframe,\n",
    "    save results in a new column and return an updated dataframe.\n",
    "    \"\"\"\n",
    "    grouped_data = sentiment_data_for_each_msg.groupby(\n",
    "        [sentiment_data_for_each_msg['from_id'].fillna('-'),\n",
    "         sentiment_data_for_each_msg['dialog ID'],\n",
    "         sentiment_data_for_each_msg['subdialog_id']])\n",
    "    data['avg_subdialog_sentiment'] = grouped_data['msg_sentiment'].transform(lambda x: x.mean().round(3))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_hour_and_dayoweek(data):\n",
    "    \"\"\"\n",
    "    Add hour and day of the week column to a dataframe\n",
    "\n",
    "    Return updated dataframe\n",
    "    \"\"\"\n",
    "    data[['hour_of_day', 'day_of_week']] = data.apply(lambda x: get_day_and_hour(x['date']),\n",
    "                                                      axis=1,\n",
    "                                                      result_type=\"expand\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def analyze_sentiment_for_hour_and_dayoweek(data_with_hour_and_week):\n",
    "    \"\"\"\n",
    "    Calculate sentiment by day of the week and hour\n",
    "    for each user.\n",
    "\n",
    "    Sentiment for a particular message is the average sentiment\n",
    "    for a message in a particular dialog and subdialog\n",
    "\n",
    "    Return a dataframe ready for visualization\n",
    "    \"\"\"\n",
    "    grouped_by_date = data_with_hour_and_week.groupby(['from_id', 'day_of_week', 'hour_of_day'])\n",
    "    finalized_data = grouped_by_date['avg_subdialog_sentiment'].mean().round(3).reset_index()\n",
    "\n",
    "    return finalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEmptyDataError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-5cc76ab23228>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPATH_TO_SAVE_GENERAL_DF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# user id to analyse sentiment of messages,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# you can get it from your general dialogs csv file in PATH_TO_SAVE_GENERAL_DF columns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mUSER_ID\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m331192040\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[1;32m    683\u001B[0m         )\n\u001B[1;32m    684\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 685\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    686\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    687\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 457\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    458\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    893\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    894\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 895\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    896\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    897\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1133\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1134\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"c\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1135\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1136\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1137\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1915\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"usecols\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1916\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1917\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1918\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1919\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mEmptyDataError\u001B[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(PATH_TO_SAVE_GENERAL_DF)\n",
    "\n",
    "# user id to analyse sentiment of messages,\n",
    "# you can get it from your general dialogs csv file in PATH_TO_SAVE_GENERAL_DF columns\n",
    "USER_ID = 331192040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tonality_dict_ua = get_ua_tonality_dict_combined()\n",
    "tonality_dict_ru = get_ru_tonality_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds 'sentiment' column to a df\n",
    "sentiment_data_for_each_msg = add_dialog_sentiment(data)\n",
    "\n",
    "# This adds 'avg_subdialog_sentiment' column to a df\n",
    "avg_sentiment_data = calculate_avg_subdialog_sentiment(sentiment_data_for_each_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds 'hour_of_day' and 'day_of_week' columns to a df\n",
    "data_with_hour_and_week = add_hour_and_dayoweek(avg_sentiment_data)\n",
    "\n",
    "# This combines data for a specific day of a week and hour, ready to be visualized\n",
    "final_sentiment_data = analyze_sentiment_for_hour_and_dayoweek(data_with_hour_and_week)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_user_sentiment(sentiment_data, user_id):\n",
    "    \"\"\"\n",
    "    Visualize sentiment data for a specific user\n",
    "    in a timeline (linear graph), starting at 00:00 on Monday,\n",
    "    finishing at 23:59 on Sunday\n",
    "    \"\"\"\n",
    "\n",
    "    user_sent_data = sentiment_data[sentiment_data['from_id'] == user_id]\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=7,\n",
    "                   subplot_titles=(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\n",
    "                                   \"Friday\", \"Saturday\", \"Sunday\"),\n",
    "                   shared_yaxes=True,\n",
    "                   x_title=\"Hours\",\n",
    "                   y_title=\"Sentiment\",\n",
    "                   horizontal_spacing=0.005)\n",
    "\n",
    "    fig.update_layout(title_text=f\"Weekly sentiment analysis for {user_id}\", height=450)\n",
    "\n",
    "    for day_of_week in range(1, 8):\n",
    "\n",
    "        day_of_week_data = user_sent_data.query(f'day_of_week == {day_of_week}')\n",
    "        hourly_data = dict(zip(day_of_week_data.hour_of_day, day_of_week_data.avg_subdialog_sentiment))\n",
    "        hourly_data_dict = {hour : 0 for hour in range(24)}\n",
    "        hourly_data_dict.update(hourly_data)\n",
    "\n",
    "        fig.add_scatter(x=list(hourly_data_dict.keys()),\n",
    "                        y=list(hourly_data_dict.values()),\n",
    "                        name=get_week_day_from_number(day_of_week),\n",
    "                        row=1, col=day_of_week)\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_user_sentiment(final_sentiment_data, USER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}